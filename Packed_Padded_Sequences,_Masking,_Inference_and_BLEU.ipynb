{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Packed Padded Sequences, Masking, Inference and BLEU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8BVq0Ych2C"
      },
      "source": [
        "# !pip install torchtext==0.5\n",
        "# !python -m spacy download de_core_news_sm\n",
        "# !python -m spacy download en_core_web_sm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1wMYUEf49I"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random, math, time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpJmt3ufgWyG"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed=(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbAtaoTDgjlj"
      },
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XBXKevgvqc"
      },
      "source": [
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_-acghXheo_"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True,\n",
        "            include_lengths = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en,\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJiBps0Bh7Ra",
        "outputId": "b7f29cee-5aeb-4367-a73b-8ebbc492f3ec"
      },
      "source": [
        "# loading the data\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 656kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 175kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 165kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mWwfZgPiJod"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMh-2iWoiOAl"
      },
      "source": [
        "BATCH_SIZE = 128   \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size =  BATCH_SIZE,\n",
        "    sort_within_batch = True, # sort sentences in each batch\n",
        "    sort_key = lambda x: len(x.src), # in descending order according to their length\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMthUjRid_F"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [batch size]\n",
        "\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "    #need to explicility put lengths on cpu\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "\n",
        "    packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "    #packed_outputs -> padded sequence containing all the hidden states\n",
        "    #hidden is now from the final non-padded element in the batch\n",
        "\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "    #outputs -> non-packed sequence, all hidden states obtained\n",
        "\n",
        "    #outputs = [src len, batch size, hid dim * num directions]\n",
        "    #hidden = [n layers * num directions, batch size, hid dim]\n",
        "\n",
        "    #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "    #outputs are always from the last layer\n",
        "\n",
        "    #hidden [-2, :, :] -> the last of the forwards RNN\n",
        "    #hidden [-1, :, :] -> the last of the backwards RNN\n",
        "\n",
        "    #initial decoder hidden state is the final hidden state of the forwards and backwards of the encoder\n",
        "    #encoder RNNs fed through a linear layer\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))\n",
        "\n",
        "    #outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    return outputs, hidden\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q-vYwOUqwor"
      },
      "source": [
        "# Attention and Masking\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    #hidden = [batch size, dec hid dim] -> hidden state from the decoder\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    #repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "    #hidden [batch size, src len, dec hid dim]\n",
        "    #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "    #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "    attention = self.v(energy).squeeze(2)\n",
        "    #attention = [batch size, src len]\n",
        "\n",
        "    # masking the attention values before softmax\n",
        "    attention = attention.masked_fill(mask==0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IRUWPpds-9n"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "\n",
        "    self.fc_out = nn.Linear((enc_hid_dim*2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    #input = [batch size]\n",
        "    #hidden = [batch size, dec hid dim]\n",
        "    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "    #mask [ batch size, src len]\n",
        "\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    #embedded = [1, batch size, emb dim]\n",
        "\n",
        "    a = self.attention(hidden, encoder_outputs, mask)\n",
        "    #a = [batch size, src len]\n",
        "\n",
        "    a = a.unsqueeze(1)\n",
        "    #a = [batch size, 1, src len]\n",
        "\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "\n",
        "    weighted = torch.bmm(a, encoder_outputs)\n",
        "    #weighted = [batch size, 1, enc hid dim * 2]\n",
        "\n",
        "    weighted = weighted.permute(1, 0, 2)\n",
        "    #weighted = [1, batch size, enc hid dim * 2]\n",
        "\n",
        "    rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "    #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "    output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "    #output = [seq len, batch size, dec hid dim * n directions]\n",
        "    #hidden [ n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "    #seq len, n layers and n directions will always be 1 here:\n",
        "    #output = [1, batch size, dec hid dim]\n",
        "    #hidden = [1, batch size, dec hid dim]\n",
        "    # this means that output == hidden\n",
        "    assert (output == hidden).all()\n",
        "\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "\n",
        "    predictions = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "    #predictions = [batch size, output dim]\n",
        "    return predictions, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OacMIEavkKl"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  # 1 when the idx != the pad idx\n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "    #src = [src len, batch size]\n",
        "    #src_len = [bathc size]\n",
        "    #trg = [trg len, batch size]\n",
        "    #teacher_forcing_ratio = the probability to use teacher forcing \n",
        "    \n",
        "    batch_size = src.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    #tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
        "\n",
        "    #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "    #hidden is the final forward and backward hidden states, pass through a linear layer\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "\n",
        "    # first input to the decoder is the <sos> tokens\n",
        "    input = trg[0, :]\n",
        "    mask = self.create_mask(src)\n",
        "\n",
        "    #mask = [batch size, src len]\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "      #insert input token embedding, pervious hidden state, all encoder hidden states and mask\n",
        "      #receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "\n",
        "      #place predictions in a tensor holding predictions\n",
        "      outputs[t] = output\n",
        "\n",
        "      #decide if we going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "      #get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = trg[t] if teacher_force else top1\n",
        "    return outputs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnt9ZHWVyWWF"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "DEC_HID_DIM = 512\n",
        "ENC_HID_DIM = 512\n",
        "ENC_DROPOUT = .5\n",
        "DEC_DROPOUT = .5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqHPnVoozCCz",
        "outputId": "304409a4-f294-466a-a185-90b5e61af47f"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(1280, 512)\n",
              "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RzaRD8UzsW3",
        "outputId": "1fe344a3-efdb-46d6-d3bf-8859d1f039d3"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 20,518,917 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvHnZ-hu0JRJ"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djGQYYo40XHS"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "\n",
        "    src, src_len = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src, src_len, trg)\n",
        "    #trg = [trg len, batch size]\n",
        "    #output = [trg len, batch size, output dim]\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpgz14EG1Xr2"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TufSCpaw1juw"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICQtVUcf1ll3",
        "outputId": "946cb089-1f16-4bdd-9bdd-bf1bca81e8ea"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 34s\n",
            "\tTrain Loss: 5.022 | Train PPL: 151.698\n",
            "\t Val. Loss: 4.775 |  Val. PPL: 118.566\n",
            "Epoch: 02 | Time: 0m 34s\n",
            "\tTrain Loss: 4.029 | Train PPL:  56.189\n",
            "\t Val. Loss: 4.055 |  Val. PPL:  57.708\n",
            "Epoch: 03 | Time: 0m 34s\n",
            "\tTrain Loss: 3.263 | Train PPL:  26.122\n",
            "\t Val. Loss: 3.538 |  Val. PPL:  34.387\n",
            "Epoch: 04 | Time: 0m 34s\n",
            "\tTrain Loss: 2.771 | Train PPL:  15.981\n",
            "\t Val. Loss: 3.392 |  Val. PPL:  29.718\n",
            "Epoch: 05 | Time: 0m 34s\n",
            "\tTrain Loss: 2.434 | Train PPL:  11.405\n",
            "\t Val. Loss: 3.223 |  Val. PPL:  25.108\n",
            "Epoch: 06 | Time: 0m 34s\n",
            "\tTrain Loss: 2.098 | Train PPL:   8.147\n",
            "\t Val. Loss: 3.219 |  Val. PPL:  25.001\n",
            "Epoch: 07 | Time: 0m 34s\n",
            "\tTrain Loss: 1.886 | Train PPL:   6.590\n",
            "\t Val. Loss: 3.182 |  Val. PPL:  24.102\n",
            "Epoch: 08 | Time: 0m 34s\n",
            "\tTrain Loss: 1.684 | Train PPL:   5.389\n",
            "\t Val. Loss: 3.298 |  Val. PPL:  27.056\n",
            "Epoch: 09 | Time: 0m 34s\n",
            "\tTrain Loss: 1.545 | Train PPL:   4.687\n",
            "\t Val. Loss: 3.262 |  Val. PPL:  26.106\n",
            "Epoch: 10 | Time: 0m 34s\n",
            "\tTrain Loss: 1.374 | Train PPL:   3.952\n",
            "\t Val. Loss: 3.333 |  Val. PPL:  28.034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTKtfVYy1rUm",
        "outputId": "de32edcb-f827-4e9f-ed27-5dbcab2e1fae"
      },
      "source": [
        "model.load_state_dict(torch.load('sample_data/tut4-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.181 | Test PPL:  24.075 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_i4Sb9e9Mnv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGEUI1y9MhS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}